
# LLM

| **Day**    | **Tasks**                                                                                                                                                                                                                                                                                                                                                                 |
| ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Day 1 ** | ==ğŸ“– Read **"Attention Is All You Need" (Vaswani et al.)** carefully.==  <br>ğŸ“– ==Go through **"The Illustrated Transformer" (Jay Alammar)** to visually understand the architecture.==                                                                                                                                                                                   |
| **Day 2 ** | ==ğŸ“– Study **BERT** architecture (focus on Pre-training objectives, Masked LM + Next Sentence Prediction).==  <br>ğŸ“– ==Begin studying **GPT architecture** (attention masks, autoregression).==<br>                                                                                                                                                                       |
| **Day 3 ** | ==ğŸ“– Study **LLaMA architecture** and its differences from GPT and BERT.==  <br>==ğŸ› ï¸ Start **implementing self-attention from scratch** in PyTorch (no full Transformer yet, just self-attention module).==<br>==- stanford language modelling lec 1==                                                                                                                   |
| **Day 4 ** | ğŸ“– ==Study **Tokenization techniques**:==  <br>==- Byte-Pair Encoding (BPE)==  <br>==- WordPiece==  <br>- ==SentencePiece== (read original papers or tutorials)  <br>==ğŸ“– Skim Hugging Face Course (Tokenization chapter).==<br>==- stanford language modelling lec 1==                                                                                                   |
| **Day 5 ** | ==ğŸ› ï¸ Experiment:==  <br>==- Build tokenizers using `tiktoken` and ğŸ¤— `tokenizers`.==  <br>==- **Train your own custom tokenizer** on a small dataset.  [cs336](https://youtu.be/SQ3fZ1sAqXI?si=7nmkSUDjAXtec_Do&t=4627)==<br>==ğŸ“Œ Watch Andrej Karpathyâ€™s "GPT from Zero to Hero" lecture (at least half).==                                                             |
| **Day 6 ** | ==âœ… **Implement a Mini-Transformer (small GPT-2)** from scratch in PyTorch (use previous self-attention code).==  <br>==ğŸ“– Watch the remaining Karpathy lecture + optionally Fast.ai NLP transformer lessons.==                                                                                                                                                           |
| **Day 7 ** | ==ğŸ” **Pending Task Completion** (catch up on anything missed).==  <br>==- stanford language modelling lec next==<br>==ğŸ” **Full Recap**:==  <br>==- Summarize key learnings from Attention paper, BERT, GPT, LLaMA.==  <br>==- Review code implementations (self-attention, MiniTransformer).==  <br>==- Reflect: Where are you still confused? Make notes for Week 2.== |


# DL + Maths
Got it! Here's your updated schedule, incorporating the **7-day PyTorch deep learning basics** from your Udemy course **before** starting the original DLOps plan (so DLOps now begins from Day 8):

---

| **Day**   | **Topics**                     | **Details**                                                                                 |
| --------- | ------------------------------ | ------------------------------------------------------------------------------------------- |
| ==**Day 1**== | ==ğŸ”¥ PyTorch Basics â€“ Part 1== | ==- Tensors, basic tensor ops, autograd - Numpy vs PyTorch - Scalars, vectors, and shapes== |
| ==**Day 2**== | ==ğŸ”¥ PyTorch Basics â€“ Part 2==     | ==- `nn.Module`, forward pass, loss functions - Training loops==                                |
| ==**Day 3**== | ==ğŸ”¥ PyTorch Basics â€“ Part 3==     | ==- Optimizers (`SGD`, `Adam`) - DataLoaders and batching - Model evaluation==                  |
| ==**Day 4**== | ==ğŸ”¥ PyTorch Basics â€“ Part 4==     | ==- GPU vs CPU, `.cuda()` vs `.to()` - Saving/loading models==                                  |
| ==**Day 5**== | ==ğŸ”¥ PyTorch Basics â€“ Part 5==     | ==- Custom datasets and transforms - Image loading using `torchvision`==                        |
| ==**Day 6**== | ==ğŸ”¥ PyTorch Basics â€“ Part 6==     | ==- `nn.Sequential` and modularity - Weight init, learning rate schedulers==                    |
| ==**Day 7**== | ==ğŸ”¥ PyTorch Basics â€“ Part 7==     | ==- TensorBoard integration - Debugging tips and common pitfalls==                              |
|           |                                |                                                                                             |


---

Let me know if you'd like to extend this for the next weeks (e.g. offloading, MoE models, Deepspeed, etc.).