[# Mathematics for Machine Learning Tutorial (3 Complete Courses in 1 video)](https://www.youtube.com/watch?v=0z6AhrOSrRs)


http://www.math.utah.edu/~zwick/Classes/Fall2012_2270/


[# The Complete Mathematics of Neural Networks and Deep Learning](https://www.youtube.com/@adamdhalla1270)


https://www.youtube.com/channel/UCcAtD_VYwcYwVbTdvArsm7w

https://medium.com/analytics-vidhya/mathematics-for-ml-is-difficult-7bbe1287105b
https://www.youtube.com/c/ProfJeffreyChasnov



![[Pasted image 20250826165258.png]]

# 3blue 1 brown 

### linear algebra
https://www.3blue1brown.com/topics/linear-algebra

- [x] Vectors: What even are they?
    - [x] Multiple interpretations for vectors
    - [x] Operations on vectors
- [x] Linear combinations, span, and basis vectors
    - [x] Span and linear combinations
    - [x] Linear dependence
- [x] Linear transformations and matrices
    - [x] Matrices as transformations
    - [x] Visualizing matrix actions
- [x] Matrix multiplication as composition
    - [x] Visual intuition for matrix multiplication
    - [x] Successive linear transformations
- [x] Three-dimensional linear transformations
    - [x] 3×3 matrices transforming 3D space
- [x] The determinant
    - [x] Visual intuition for the determinant
- [x] Inverse matrices, column space, and null space
    - [x] Column space and null space (visual)
    - [x] Matrix inverses
- [x] Nonsquare matrices as transformations between dimensions
    - [x] Transformations with nonsquare matrices
- [x] Dotproducts and duality
    - [x] Meaning of the dot product
    - [x] Formula and visualization

- [x] Cross products
    - [x] Visualizing the cross product

- [x] Cross products in the light of linear transformations
    - [x] Connection between cross products and determinants

- [ ] Cramer's rule, explained geometrically
    - [ ] What is Cramer's rule?
    - [ ] Geometric explanation

- [x] Change of basis
    - [x] What is a change of basis?
    - [x] How to perform a change of basis

- [x] Eigenvectors and eigenvalues
    - [x] Understanding eigenvalues/eigenvectors

- [ ] A quick trick for computing eigenvalues
    - [ ] Fast computation for 2×2 matrices

- [x] Abstract vector spaces
    - [x] General idea of vector spaces
    - [x] Vectors as more than just arrows/numbers






# khan academy

https://www.khanacademy.org/math/linear-algebra?source=post_page-----7bbe1287105b---------------------------------------

https://www.khanacademy.org/math/multivariable-calculus?source=post_page-----7bbe1287105b---------------------------------------

done https://www.khanacademy.org/math/statistics-probability?source=post_page-----7bbe1287105b---------------------------------------

# intro to prob


# stats110

[stats 110](https://www.youtube.com/watch?v=KbB0FjPg0mw&list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo&index=1)


- [x] Lecture 1
- [x] Lecture 2
- [x] Lecture 3
- [ ] Lecture 4
- [ ] Lecture 5
- [ ] Lecture 6
- [ ] Lecture 7
- [ ] Lecture 8
- [ ] Lecture 9
- [ ] Lecture 10
- [ ] Lecture 11
- [ ] Lecture 12
- [ ] Lecture 13
- [ ] Lecture 14
- [ ] Lecture 15
- [ ] Lecture 16
- [ ] Lecture 17
- [ ] Lecture 18
- [ ] Lecture 19
- [ ] Lecture 20
- [ ] Lecture 21
- [ ] Lecture 22
- [ ] Lecture 23
- [ ] Lecture 24
- [ ] Lecture 25
- [ ] Lecture 26
- [ ] Lecture 27
- [ ] Lecture 28
- [ ] Lecture 29
- [ ] Lecture 30
- [ ] Lecture 31
- [ ] Lecture 32
- [ ] Lecture 33
- [ ] Lecture 34
- [ ] Lecture 35



# maths4ml

### Part I: Mathematical Foundations

- [x] Chapter 1: Introduction and Motivation
    - [x] 1.1 Finding Words for Intuitions
    - [x] 1.2 Two Ways to Read This Book
    - [x] 1.3 Exercises and Feedback

- [ ] Chapter 2: Linear Algebra
    - [x] 2.1 Systems of Linear Equations
    - [x] 2.2 Matrices
    - [x] 2.3 Solving Systems of Linear Equations
    - [x] 2.4 Vector Spaces
    - [x] 2.5 Linear Independence
    - [x] 2.6 Basis and Rank
    - [ ] 2.7 Linear Mappings
    - [ ] 2.8 Affine Spaces
    - [ ] 2.9 Further Reading
    - [ ] Exercises

- [ ] Chapter 3: Analytic Geometry
    - [ ] 3.1 Norms
    - [ ] 3.2 Inner Products
    - [ ] 3.3 Lengths and Distances
    - [ ] 3.4 Angles and Orthogonality
    - [ ] 3.5 Orthonormal Basis
    - [ ] 3.6 Orthogonal Complement
    - [ ] 3.7 Inner Product of Functions
    - [ ] 3.8 Orthogonal Projections
    - [ ] 3.9 Rotations
    - [ ] 3.10 Further Reading
    - [ ] Exercises

- [ ] Chapter 4: Matrix Decompositions
    - [ ] 4.1 Determinant and Trace
    - [ ] 4.2 Eigenvalues and Eigenvectors
    - [ ] 4.3 Cholesky Decomposition
    - [ ] 4.4 Eigendecomposition and Diagonalization
    - [ ] 4.5 Singular Value Decomposition
    - [ ] 4.6 Matrix Approximation
    - [ ] 4.7 Matrix Phylogeny
    - [ ] 4.8 Further Reading
    - [ ] Exercises

- [ ] Chapter 5: Vector Calculus
    - [ ] 5.1 Differentiation of Univariate Functions
    - [ ] 5.2 Partial Differentiation and Gradients
    - [ ] 5.3 Gradients of Vector-Valued Functions
    - [ ] 5.4 Gradients of Matrices
    - [ ] 5.5 Useful Identities for Computing Gradients
    - [ ] 5.6 Backpropagation and Automatic Differentiation
    - [ ] 5.7 Higher-Order Derivatives
    - [ ] 5.8 Linearization and Multivariate Taylor Series
    - [ ] 5.9 Further Reading
    - [ ] Exercises

- [ ] Chapter 6: Probability and Distributions
    - [ ] 6.1 Construction of a Probability Space
    - [ ] 6.2 Discrete and Continuous Probabilities
    - [ ] 6.3 Sum Rule, Product Rule, and Bayes’ Theorem
    - [ ] 6.4 Summary Statistics and Independence
    - [ ] 6.5 Gaussian Distribution
    - [ ] 6.6 Conjugacy and the Exponential Family
    - [ ] 6.7 Change of Variables/Inverse Transform
    - [ ] 6.8 Further Reading
    - [ ] Exercises

- [ ] Chapter 7: Continuous Optimization
    - [ ] 7.1 Optimization Using Gradient Descent
    - [ ] 7.2 Constrained Optimization and Lagrange Multipliers
    - [ ] 7.3 Convex Optimization
    - [ ] 7.4 Further Reading
    - [ ] Exercises

### Part II

- [ ] Chapter 8: Central Machine Learning Problems
    - [ ] 8.1 When Models Meet Data
    - [ ] 8.2 Data, Models, and Learning
    - [ ] 8.3 Empirical Risk Minimization
    - [ ] 8.4 Parameter Estimation
    - [ ] 8.5 Probabilistic Modeling and Inference
    - [ ] 8.6 Directed Graphical Models
    - [ ] Model Selection

- [ ] Chapter 9: Linear Regression
    - [ ] 9.1 Problem Formulation
    - [ ] 9.2 Parameter Estimation
    - [ ] 9.3 Bayesian Linear Regression
    - [ ] 9.4 Maximum Likelihood as Orthogonal Projection
    - [ ] 9.5 Further Reading

- [ ] Chapter 10: Dimensionality Reduction with Principal Component Analysis
    - [ ] 10.1 Problem Setting
    - [ ] 10.2 Maximum Variance Perspective
    - [ ] 10.3 Projection Perspective
    - [ ] 10.4 Eigenvector Computation and Low-Rank Approximations
    - [ ] 10.5 PCA in High Dimensions
    - [ ] 10.6 Key Steps of PCA in Practice
    - [ ] 10.7 Latent Variable Perspective
    - [ ] 10.8 Further Reading

- [ ] Chapter 11: Density Estimation with Gaussian Mixture Models
    - [ ] 11.1 Gaussian Mixture Model
    - [ ] 11.2 Parameter Learning via Maximum Likelihood
    - [ ] 11.3 EM Algorithm
    - [ ] 11.4 Latent-Variable Perspective
    - [ ] 11.5 Further Reading

- [ ] Chapter 12: Classification with Support Vector Machines
    - [ ] 12.1 Separating Hyperplanes
    - [ ] 12.2 Primal Support Vector Machine
    - [ ] 12.3 Dual Support Vector Machine
    - [ ] 12.4 Kernels
    - [ ] 12.5 Numerical Solution
    - [ ] 12.6 Further Reading






# linear algerbra gilbert strang

http://web.mit.edu/18.06



- [x] datascience
	- [x] d1
	- [x] d2
	- [x] d3
	- [x] d4
	- [x] d5
	- [x] d6
	- [x] d7