1. attention is all you need
 2. https://arxiv.org/pdf/1810.04805 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
 3. LLaMA: Open and Efficient Foundation Language Models
 4. Improving Language Understanding by Generative Pre-Training
