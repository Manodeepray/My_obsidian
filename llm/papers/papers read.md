1. attention is all you need
 2. https://arxiv.org/pdf/1810.04805 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
