[https://arxiv.org/pdf/2510.17558](https://arxiv.org/pdf/2510.17558)

[https://thinkingmachines.ai/blog/lora/](https://thinkingmachines.ai/blog/lora/)

[https://www.linkedin.com/posts/sumanth077_antonio-gulli-released-a-free-400-page-book-activity-7383477090458877952-zILg/](https://www.linkedin.com/posts/sumanth077_antonio-gulli-released-a-free-400-page-book-activity-7383477090458877952-zILg/)

[https://gpuengineering.com/?trk=feed_main-feed-card-text](https://gpuengineering.com/?trk=feed_main-feed-card-text)

[https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf](https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf)

[https://quantshub.com/](https://quantshub.com/)

[https://www.youtube.com/watch?v=XR-uPPCXGAs](https://www.youtube.com/watch?v=XR-uPPCXGAs)

read this shit

[https://arxiv.org/abs/2507.14805](https://arxiv.org/abs/2507.14805)

[https://arxiv.org/pdf/2410.21228](https://arxiv.org/pdf/2410.21228)

[https://scholar.google.com/citations?user=LkVtZkQAAAAJ&hl=pl](https://scholar.google.com/citations?user=LkVtZkQAAAAJ&hl=pl) adam pasczke

[https://blog.fotiecodes.com/orpo-dpo-and-ppo-optimizing-models-for-human-preferences-cm38nqzki000z09l23tay04ev](https://blog.fotiecodes.com/orpo-dpo-and-ppo-optimizing-models-for-human-preferences-cm38nqzki000z09l23tay04ev)

- [ ] monolith [https://arxiv.org/pdf/2209.07663](https://arxiv.org/pdf/2209.07663)
- [ ] efficiently scaling llm inference [https://arxiv.org/pdf/2211.05102](https://arxiv.org/pdf/2211.05102)
- [ ] [https://medium.com/@oladokunjoseph2/simplifying-tiktoks-monolith-recommendation-algorithm-a7017f7ca3ab](https://medium.com/@oladokunjoseph2/simplifying-tiktoks-monolith-recommendation-algorithm-a7017f7ca3ab)
- [ ] spec decoding [https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/](https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/)
- [ ] post training [https://pytorch.org/blog/a-primer-on-llm-post-training/?utm_campaign=4079123-PyTorch Blog Post Promotion&utm_content=345149081&utm_medium=social&utm_source=twitter&hss_channel=tw-776585502606721024](https://pytorch.org/blog/a-primer-on-llm-post-training/?utm_campaign=4079123-PyTorch%20Blog%20Post%20Promotion&utm_content=345149081&utm_medium=social&utm_source=twitter&hss_channel=tw-776585502606721024)
- [ ] [https://huggingface.co/spaces/transformers-community/Transformers-tenets](https://huggingface.co/spaces/transformers-community/Transformers-tenets)
- [ ] [https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high-level_overview](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high-level_overview)
- [ ] [https://huggingface.co/blog/faster-transformers](https://huggingface.co/blog/faster-transformers)
- [ ] [https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook#understanding-what-works-evaluation](https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook#understanding-what-works-evaluation)
- [ ] [https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=fp8_pretraining](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=fp8_pretraining)
- [ ] [https://www.neelnanda.io/blog/mini-blog-post-9-what-it-means-to-optimise](https://www.neelnanda.io/blog/mini-blog-post-9-what-it-means-to-optimise)
- [ ] [https://arxiv.org/html/2409.03752v3](https://arxiv.org/html/2409.03752v3)
- [ ] [https://arxiv.org/abs/2407.15360](https://arxiv.org/abs/2407.15360)
- [ ] [https://docs.google.com/document/d/163AgMuYKvccIfmmQ3_bMYFnMt_9-_wUk2wDa9jesyD0/edit?tab=t.0](https://docs.google.com/document/d/163AgMuYKvccIfmmQ3_bMYFnMt_9-_wUk2wDa9jesyD0/edit?tab=t.0)
- [ ] [https://andrewkchan.dev/posts/yalm.html](https://andrewkchan.dev/posts/yalm.html)