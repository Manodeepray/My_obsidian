re attentions https://github.com/ttt496/vit-pytorch/blob/main/vit_pytorch/deepvit.py

https://medium.com/@zhonghong9998/attention-mechanisms-in-deep-learning-enhancing-model-performance-32a91006092a

https://medium.com/@isaakmwangi2018/intro-to-differential-transformers-a-new-attention-mechanisms-for-large-language-models-llms-9d977b5857ae

**CSWin Transformer**

This efficient Transformer-based backbone uses a new technique called "Cross-Shaped Window self-attention" to analyze different parts of an image simultaneously.Â CSWin Transformer has achieved excellent performance in benchmark tasks.