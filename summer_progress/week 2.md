					

## other courses




## LLMs


### ğŸ“… Week 2: Day-Wise Timeline â€“ Training & Optimization

| **Day**   | **Focus Area**                      | **Resources & Topics Covered**                                                                                                                                                                            | **Outcomes / Experiments**                                                                                 |     |
| --------- | ----------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- | --- |
| **Day 1** | Transformer Training Fundamentals   | ==ğŸ¥ [Training a Transformer from Scratch (Umar Jamil)](https://www.youtube.com/watch?v=ISNdQcPhsts)==  <br>==ğŸ“– Review model architecture + training loop==  <br>ğŸ“– Study _Scaling Laws_ (Kaplan et al.) | âœ… ==Refreshed Transformer training pipeline==  <br>âœ… Took notes on compute vs performance scaling insights |     |
| **Day 2** | Data Pipeline for LLMs              | ==ğŸ“– Read about data **sharding** and **streaming**==<br>ğŸ“– ==Explored Hugging Face `datasets` and `WebDataset` formats==  <br>ğŸ“Œ Skimmed data preprocessing tutorials for large corpora                  | ==âœ… Built a streaming dataset loader==<br>âœ… Preprocessed sample dataset for next day's training            |     |
| **Day 3** | Training Optimizations              | ğŸ“– Study:  <br>â€¢ **Gradient Checkpointing**  <br>â€¢ **Mixed Precision Training** (FP16/BF16)  <br>â€¢ **ZeRO, FSDP, DeepSpeed**  <br>ğŸ“Œ Read docs/tutorials from Microsoft and Hugging Face                  | âœ… Enabled mixed precision in PyTorch  <br>âœ… Tested DeepSpeed config on local experiment                    |     |
| **Day 4** | Efficient Batching Techniques       | ğŸ“– Study batching techniques:  <br>â€¢ **Prefetching**, **Bucketing**, **Packing**  <br>ğŸ“Œ Watched related Stanford CS25 snippets  <br>ğŸ“Œ Skimmed ğŸ¤— `Trainer` batching logic                               | âœ… Implemented smart batching in tokenizer pipeline  <br>âœ… Benchmarked different strategies                 |     |
| **Day 5** | Model Training: TinyStories Dataset | ğŸ› ï¸ Train GPT-like model on **TinyStories**  <br>ğŸ“– Review GPT-1 architecture for model choice and config                                                                                                 | âœ… Successfully trained GPT-style model  <br>âœ… Logged training curves and attention patterns                |     |
| **Day 6** | Fine-Tuning with DeepSpeed/FSDP     | ğŸ“– Fine-tuning strategy for small datasets  <br>ğŸ“Œ Setup DeepSpeed with offload + FSDP  <br>ğŸ“Œ Traced training memory usage and GPU utilization                                                           | âœ… Fine-tuned TinyStories model on a small custom dataset using DeepSpeed                                   |     |
| **Day 7** | LoRA, QLoRA, Quantization           | ğŸ“– Read:  <br>â€¢ _LoRA: Low-Rank Adaptation_  <br>â€¢ _GPTQ: Quantization of LLMs_  <br>ğŸ“Œ Try integrating LoRA into PyTorch training loop  <br>ğŸ“Œ Notes from Stanford CS25 & Hugging Face blogs             | âœ… Prototype LoRA-based fine-tuning loop  <br>âœ… Wrote summary + comparison of LoRA, QLoRA, GPTQ methods     |     |








## dl + maths

| **Day 8**  | ğŸ“Œ What is DLOps?                                            | - Differences from MLOps - Real-world applications and role in AI lifecycle                                                              |
| ---------- | ------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------- |
| **Day 9**  | ğŸ“Œ Case Studies in DLOps                                     | - Study Uber Michelangelo, Meta FBLearner - Understand real industry pipelines                                                           |
| **Day 10** | ğŸ“Œ DL Math - Linear Algebra I                                | - Scalars, vectors, matrices, tensors - Operations: addition, dot product, transpose - Hands-on: NumPy matrix operations - HF Agents vid |
| **Day 11** | ğŸ“Œ Cloud & On-Prem Infrastructure                            | - GPU vs TPU vs CPU overview - AWS SageMaker, GCP Vertex AI, Azure ML overview - HF Agents vid                                           |
| **Day 12** | ğŸ“Œ DL Math - Linear Algebra II                               | - Eigenvalues, eigenvectors, SVD, norms - PCA and backprop intuition - HF Agents vid                                                     |
| **Day 13** | ğŸ“Œ Containerization & Virtualization Basics (Part 1)         | - Docker: images, containers, volumes, networking - Write Dockerfile, set up containers - Set up NVIDIA CUDA & cuDNN                     |
| **Day 14** | ğŸ“Œ Containerization & Virtualization Basics (Part 2) + Recap | - Dockerize a DL model - Deploy on Kubernetes (GKE, EKS, AKS) - Full recap and catch-up - HF Agents vid                                  |
## ğŸ—“ï¸ Week 1: Linear Algebra + Analytic Geometry

|Day|Topic|Chapter Sections|Time (hrs)|
|---|---|---|---|
|Day 1|Intro + Systems of Equations|2.1 â€“ 2.3|2.5|
|Day 2|Solving + Vector Spaces|2.4 â€“ 2.5|2.5|
|Day 3|Independence + Basis + Rank|2.6 â€“ 2.7|2.5|
|Day 4|Linear Mappings + Affine Spaces|2.8 â€“ 2.9|2.5|
|Day 5|Exercises (Linear Algebra)|â€“|2.5|
|Day 6|Norms, Inner Products, Distances|3.1 â€“ 3.3|2.5|
|Day 7|Catch-up + Light Review|Buffer / Rest|1â€“2|
