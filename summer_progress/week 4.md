|**Day**|**Focus Area**|**Activities**|**Deliverables**|
|---|---|---|---|
|**Day 1**|🧠 RL for LLMs: RLHF + DPO|- Study RLHF: process, components, real-world use cases- Understand DPO (Direct Preference Optimization) from papers/videos- Read reward modeling pipeline|Summary notes comparing RLHF and DPO; diagram of RLHF pipeline|
|**Day 2**|🔬 Long Context Mechanisms|- Read RetNet, RWKV, Hyena papers- Understand how long context modeling differs from vanilla transformers|Notes on memory-efficient attention; key insights from each paper|
|**Day 3**|🧪 Long-Context Model Exploration|- Try long-context LLMs (Mistral, Claude 3, Gemma)- Evaluate context length, latency, and quality of completions using a few sample prompts|Prompt-response logs; context-length performance summary|
|**Day 4**|🛠️ Fine-tuning with DPO|- Watch YouTube tutorial on VLM fine-tuning using DPO- Try a toy fine-tuning task using `trl` or `nanoGPT` + preference data|Fine-tune a model on small dataset; document DPO training code|
|**Day 5**|📄 Resume Optimization|- Update your resume: highlight quantization, inference, long context work- Add links to blog posts, GitHub repos|Finalized resume tailored to LLM efficiency & optimization work|
|**Day 6**|🚀 Internship Applications|- Apply to Hugging Face, EleutherAI, OpenAI, LLM-focused startups- Prepare email drafts or cover letters|At least 3 internship applications submitted|
|**Day 7**|🌐 Open-Source & Blog|- Start or contribute to an open-source LLM tool (quantization, inference, etc.)- Write a blog post on what you learned during this track|GitHub repo link + blog post published or drafted|