[Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923#:~:text=Large%20language%20models%20(LLMs)%20have,rationale%20to%20infer%20the%20answer.)

[Representation noising effectively prevents harmful fine-tuning on LLMs](https://arxiv.org/abs/2405.14577)

> [Erasing Conceptual Knowledge from Language Models](https://arxiv.org/abs/2410.02760)

[Eight Methods to Evaluate Robust Unlearning in LLMs](https://arxiv.org/abs/2402.16835)

[Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?](https://arxiv.org/abs/2405.05904)

[# Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning](https://arxiv.org/abs/2405.03279)

[# Fine-grained Hallucination Detection and Editing for Language Models](https://arxiv.org/abs/2401.06855)

[# Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models](https://arxiv.org/abs/2409.17539) -  [medium blog on LoT]( https://medium.com/@techsachin/logic-of-thought-prompting-approach-leveraging-propositional-logic-to-enhance-logical-reasoning-f15fe50d909a) , [twitter post by RohanPaul](https://x.com/rohanpaul_ai/status/1842366845040398409)

[Were RNNs All We Needed](https://arxiv.org/abs/2410.01201)

15/10/24
https://x.com/tom_doerr/status/1845312432182284320?t=h7VRlvztukNewsiWGe4Dqw&s=08

https://x.com/natolambert/status/1845501056110760376?t=W_mUclOCmOUZwc6zbkCs8g&s=08

https://arxiv.org/pdf/2305.20050

https://x.com/MLStreetTalk/status/1795094089433526362?t=p7Vw-RXtKrjGJQIZ2guhdA&s=08



https://www.linkedin.com/posts/tunguz_at-the-end-of-the-last-week-dario-amodei-activity-7251571894359920640-IkXp/

https://darioamodei.com/machines-of-loving-grace

https://x.com/alxndrdavies/status/1845797243292696950?t=azNr5pHhWVc7V-Un6PainA&s=08

[A Closer Look at Machine Unlearning for Large Language Models](https://arxiv.org/abs/2410.08109)

https://huggingface.co/blog/moe


19-10

LOOKING INWARD: LANGUAGE MODELS CAN LEARN ABOUT THEMSELVES BY INTROSPECTION

Are we going MAD? Benchmarking Multi-Agent Debate between Language Models for Medical Q&A

Persuasion Games with Large Language Models


https://x.com/deedydas/status/1847311486148104686?t=t4RbW0-ZA4tmXOR673in0g&s=08


[Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning](https://arxiv.org/pdf/2410.13274)

[NEGMERGE: CONSENSUAL WEIGHT NEGATION FOR STRONG MACHINE UNLEARNING](https://arxiv.org/pdf/2410.05583)

https://github.com/facebookresearch/lingua

[MIND: MATH INFORMED SYNTHETIC DIALOGUES FOR PRETRAINING LLMS](https://arxiv.org/pdf/2410.12881)


imp
>https://x.com/gordic_aleksa/status/1846144422225797278

