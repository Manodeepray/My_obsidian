


[A Literature Survey on Domain Adaptation of Statistical Classifiers](http://www.mysmu.edu/faculty/jingjiang/papers/da_survey.pdf)

[Activation Functions in Deep Learning](https://arxiv.org/pdf/2109.14545)

[Variance-Covariance Regularization Improves Representation Learning](https://arxiv.org/pdf/2306.13292)

[TiCo: Transformation Invariance and Covariance Contrast for Self-Supervised Visual Representation Learning](https://arxiv.org/pdf/2206.10698)

PCA and others
[pca](https://www.semanticscholar.org/paper/Principal-component-analysis%3A-a-review-and-recent-Jolliffe-Cadima/5bc875d65df812f9617d8ba508c1c85f4d219b19)
[Uncertainty-Aware Principal Component Analysis](semanticscholar.org/paper/Uncertainty-Aware-Principal-Component-Analysis-GÃ¶rtler-Spinner/4bf2c0ccd75be41d36455949eb8906910404fe1c)

interpretability
[Transcoders Find Interpretable LLM Feature Circuits](https://arxiv.org/pdf/2406.11944)


layer dropping

[Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping][https://arxiv.org/pdf/2010.13369]


Normalization papers
[Transformers without Tears: Improving the Normalization of Self-Attention](https://arxiv.org/abs/1910.05895)
[Micro-Batch Training with Batch-Channel Normalization and Weight Standardization](https://arxiv.org/abs/1903.10520)

[# nGPT: Normalized Transformer with Representation Learning on the Hypersphere](https://arxiv.org/abs/2410.01131)
[# Normalization Layers Are All That Sharpness-Aware Minimization Needs](https://arxiv.org/abs/2306.04226)

[# Normalization Techniques in Training DNNs: Methodology, Analysis and Application](https://arxiv.org/abs/2009.12836)

[# The Normalization Method for Alleviating Pathological Sharpness in Wide Neural Networks](https://arxiv.org/abs/1906.02926)