- [ ] [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923#:~:text=Large%20language%20models%20(LLMs)%20have,rationale%20to%20infer%20the%20answer.)

- [ ] [Representation noising effectively prevents harmful fine-tuning on LLMs](https://arxiv.org/abs/2405.14577)

- [ ] > [Erasing Conceptual Knowledge from Language Models](https://arxiv.org/abs/2410.02760)

- [ ] [Eight Methods to Evaluate Robust Unlearning in LLMs](https://arxiv.org/abs/2402.16835)

- [ ] [Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?](https://arxiv.org/abs/2405.05904)

- [ ] [# Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning](https://arxiv.org/abs/2405.03279)

- [ ] [# Fine-grained Hallucination Detection and Editing for Language Models](https://arxiv.org/abs/2401.06855)

- [ ] [# Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models](https://arxiv.org/abs/2409.17539) -  [medium blog on LoT]( https://medium.com/@techsachin/logic-of-thought-prompting-approach-leveraging-propositional-logic-to-enhance-logical-reasoning-f15fe50d909a) 

- [ ] [twitter post by RohanPaul](https://x.com/rohanpaul_ai/status/1842366845040398409)

- [ ] [Were RNNs All We Needed](https://arxiv.org/abs/2410.01201)

15/10/24
- [ ] https://x.com/tom_doerr/status/1845312432182284320?t=h7VRlvztukNewsiWGe4Dqw&s=08

- [ ] https://x.com/natolambert/status/1845501056110760376?t=W_mUclOCmOUZwc6zbkCs8g&s=08

- [ ] https://arxiv.org/pdf/2305.20050

- [ ] https://x.com/MLStreetTalk/status/1795094089433526362?t=p7Vw-RXtKrjGJQIZ2guhdA&s=08



- [ ] https://www.linkedin.com/posts/tunguz_at-the-end-of-the-last-week-dario-amodei-activity-7251571894359920640-IkXp/

- [ ] https://darioamodei.com/machines-of-loving-grace

- [ ] https://x.com/alxndrdavies/status/1845797243292696950?t=azNr5pHhWVc7V-Un6PainA&s=08

- [ ] [A Closer Look at Machine Unlearning for Large Language Models](https://arxiv.org/abs/2410.08109)

- [ ] https://huggingface.co/blog/moe


19-10

- [ ] LOOKING INWARD: LANGUAGE MODELS CAN LEARN ABOUT THEMSELVES BY INTROSPECTION

- [ ] Are we going MAD? Benchmarking Multi-Agent Debate between Language Models for Medical Q&A

- [ ] Persuasion Games with Large Language Models


- [ ] https://x.com/deedydas/status/1847311486148104686?t=t4RbW0-ZA4tmXOR673in0g&s=08


- [ ] [Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning](https://arxiv.org/pdf/2410.13274)

- [ ] [NEGMERGE: CONSENSUAL WEIGHT NEGATION FOR STRONG MACHINE UNLEARNING](https://arxiv.org/pdf/2410.05583)

- [ ] https://github.com/facebookresearch/lingua

- [ ] [MIND: MATH INFORMED SYNTHETIC DIALOGUES FOR PRETRAINING LLMS](https://arxiv.org/pdf/2410.12881)


imp
- [ ] >https://x.com/gordic_aleksa/status/1846144422225797278



26 10 24
- [ ] [# MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models](https://arxiv.org/abs/2410.13085)

- [ ] https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea

- [ ] https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/

- [ ] [MU rag 2022](https://arxiv.org/abs/2210.02928)

- [ ] 

30-10-24

- [ ] [# Mathematical Capabilities of ChatGPT](https://arxiv.org/abs/2301.13867)


1-11-24

- [ ] https://arxiv.org/pdf/210.03833

- [ ] https://arxiv.org/html/2410.06446v1

- [ ] https://arxiv.org/abs/2406.13356

- [ ] [DO UNLEARNING METHODS REMOVE INFORMATION FROM LANGUAGE MODEL WEIGHTS?](https://arxiv.org/html/2410.08827v1)

- [ ] https://arxiv.org/abs/2410.08074

- [ ] https://arxiv.org/abs/2410.03833

- [ ] https://arxiv.org/pdf/2401.02038v2

- [ ] https://arxiv.org/abs/2410.23123


3-11-24

- [ ] [On Effects of Steering Latent Representation forÂ Large Language Model Unlearning](https://arxiv.org/html/2408.06223v1)  
- [ ] tree of thought
- [ ] [THE GEOMETRY OF CONCEPTS: SPARSE AUTOENCODER FEATURE STRUCTURE](https://arxiv.org/pdf/2410.19750) 
	