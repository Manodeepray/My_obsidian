## ------------------------------------------------------
## Official Review of Submission6962 by Reviewer xYHb

[](https://openreview.net/forum?id=s5orchdb33&noteId=WU2MdijzF4)


### Overall Score and Confidence

- **Rating:** 6 (marginally above the acceptance threshold)
    
- **Confidence:** 3 (fairly confident)
    

### Summary of the Paper

- The paper addresses the vulnerability of Large Language Models (LLMs) to **adversarial attacks** that bypass safeguards and elicit harmful responses.
    
- It identifies a shared mechanism: attacks **ablate a "refusal feature" (RF)** in the residual stream embedding space.
    
- The RF is a linear predictor of input harmfulness, discovered by Arditi et al. (2024), representing the mass mean difference in hidden representations between harmful and harmless instructions. It's crucial for safe responses.
    
- Based on this, the paper proposes **Refusal Feature Adversarial Training (ReFAT)** to enable LLMs to recognize harmful instructions and maintain robustness.
    
- Experimental results show ReFAT enhances LLM robustness against various attacks and **reduces computational overhead** compared to traditional adversarial training.
    

### Soundness, Presentation, and Contribution

- **Soundness:** 3 (good)
    
- **Presentation:** 3 (good)
    
- **Contribution:** 3 (good)
    

### Strengths

- **Innovative Method:** Extends Arditi's Refusal Feature, proves similarity between adversarial attack and RFA mechanisms through causal theory, and proposes a more efficient adversarial training method.
    
- **Objective Evaluation:** The proposed method can be objectively evaluated using metrics like attack success rate, generation performance, and efficiency. Limitations are also analyzed.
    
- **Well-Organized:** The paper is structured logically, analyzing the general mechanism of adversarial attacks in semantic space, proposing ReFAT, and supporting it with extensive experiments and supplementary materials.
    

### Weaknesses

- **Experimental Design Issue:** Using Llama-3-8B-Instruct-generated XSTest responses as the gold standard for supervised fine-tuning might violate independence principles, especially since Llama-3-8B-Instruct is also a subject of subsequent experiments.
    
- **Unclear Performance Degradation Demonstration:** The claim that rejecting direction leads to performance degradation (Section 3.1) is not well-supported by experimental figures in Appendix D, which only show "harmless example features are not centered near zero," leading to speculation rather than direct proof.
    

### Questions

- **Figure 3 (PCA visualization):** Why does the AutoDAN section have noticeably fewer experimental samples from HarmBench compared to other attack algorithms?
    
- **Harmful vs. HarmBench/AdvBench:** Section 3.1 mentions harmful samples from AdvBench (a dataset for suffix-type attacks like GCG). Intuitively, HarmBench (adversarial) and Harmful samples should be similar due to both being suffix attacks. Why do the actual results not reflect this similarity?

## AUTHOR'S RESPONSE

### **W1: Llama-3-8B-Instruct-generated XSTest responses as "gold standard"**

- **Misleading Terminology:** The term "gold-standard answers" was misleading. The 150 XSTest examples used for supervised fine-tuning are from a holdout set, completely separate from the evaluation dataset in Table 1.
	
- **Rationale for Llama-3-8B-Instruct:** Llama-3-8B-Instruct was chosen because it demonstrates near-perfect accuracy (97.2%) on XSTest (Table 1, first row).
	
- **Self-Training, Not Data Leakage:** Using predictions from a model to improve the model itself is a form of self-training and does not constitute data leakage.
	
- **Wording Change:** The sentence "We take the XSTest responses generated by Llama-3-8B-Instruct as gold-standard answers for supervised fine-tuning" will be changed to "We use the responses generated by Llama-3-8B-Instruct on this holdout sample from XSTest as references for the next-token prediction task in the supervised finetuning step."
	
- **Potential Bias & Purpose of Evaluation:** Acknowledged that using generations from one model for fine-tuning others _could_ introduce a small bias (positive or negative), but given the small sample size, this bias is likely negligible. The primary goal of the evaluation is not to compare models' relative performance, but to verify the systematic impact of adversarial training methods across model families.
	
### **W2: Rejecting Direction and Performance Degradation**

- **Observation from Early Experiments:** The degradation observed when zeroing the projection on the refusal direction was noted in early small-scale experiments.
	
- **Figure 18 & 19 Insights:** Figures 18 and 19 show that '0' is not consistently "harmless." In some cases (e.g., Layer 3 for gemma-2b-it in Fig. 19), it can be close to the "harmfulness" mode or even out of distribution (e.g., Layer 0 in both models).
	
- **Correct Approach:** Therefore, the authors maintain that setting the projection to a value squarely within the "harmless" range is the correct approach.
	
- **Wording Change:** The wording in Section 3.1 has been updated to reflect this understanding.
	

---

### **Responses to Questions**

- **Q1: Figure 3 - PCA visualization, fewer AutoDAN samples from HarmBench**
    
    - **Lower Success Rate:** The reason for fewer experimental samples from HarmBench for AutoDAN in Figure 3 is that the authors only plot cases where the attack succeeds. AutoDAN has a significantly lower success rate compared to other attacks on Llama-3-8B-Instruct (as shown in Table 1).
        
- **Q2: Section 3.1 - Harmful samples from AdvBench (suffix-type attacks)**
    
    - **Clarification on Harmful Samples:** The authors clarify that harmful samples were taken from the "Harmful Behaviours" part of the AdvBench dataset.
        
    - **Generic Red Teaming Prompts:** These harmful prompts are generic "red teaming" style prompts and are _not_ specifically designed for adversarial suffix-type attacks (e.g., they do not contain nonsensical suffices like those found by GCG).
        
    - **Original Use:** While they were originally used in a paper that developed suffix-type attacks, the prompts themselves are not inherently suffix-type. The same applies to samples taken from HarmBench.
        
    - **Request for Further Elaboration:** The authors are unsure if there was a misunderstanding and kindly ask the reviewer to elaborate if a specific phenomenon needs to be explained.
        

---


## ------------------------------------------------------

## Official Review of Submission6962 by Reviewer  SyTG

[](https://openreview.net/forum?id=s5orchdb33&noteId=WU2MdijzF4)
### **Summary:**

- **Problem:** LLMs are vulnerable to adversarial attacks, leading to harmful, sensitive, or false outputs. Attacks exploit the "Refusal Feature" (RF), making harmful inputs seem benign.
    
- **Solution:** Paper proposes **ReFAT** (Refusal Feature Adversarial Training).
    
- **ReFAT Mechanism:** Dynamically computes RF using harmful/harmless inputs. Ablates RF for harmful inputs during training.
    
- **Goal:** Trains model to make safety decisions _without_ relying on obvious malicious features, improving refusal capabilities.
    ---



**Soundness: 3 (good)**
**Presentation: 1 (poor)**
**Contribution: 3 (good)**
**Rating: 6 (marginally above the acceptance threshold)**
**Confidence: 2 

### **Strengths:**

- **Novelty:** Method is new and conceptually sound.
    
- **Approach:** Dynamically computing and ablating RF is a reasonable simulation of attacks.
    
- **RF Exploration:** Interesting exploration of refusal features; intervention mechanism seems reasonable (but needs more clarity).
    
- **Effectiveness:** Compelling evidence that ReFAT boosts model robustness across LLMs and attack types.
    

### **Weaknesses & Questions (Combined):**

- **RF Explanation (Section 3.1):**
    
    - Content isn't self-contained; hard to understand Equation 3's heuristics and physical meanings.
        
    - Curious about Equation 3's derivation and the purpose/meaning of its third term (looks like a bias).
        
    - Need more discussion on physical meanings of these formulations, as they are crucial for Sections 3.2-3.3.
        
- **rHH​ vs. rA​ (Sections 3.1 & 3.2):**
    
    - What's the difference? They seem identical.
        
    - If identical, their similar cosine similarity is obvious.
        
- **Algorithm Workflow (Section 4.2):**
    
    - Workflow unclear from description; needs more detail.
        
    - Many hyperparameters (perturbation layer, strength) lack specific configurations.
        
- **Connection to Other Alignment Methods:**
    
    - Needs discussion on connection to previous alignment methods, especially those with binary feedback.
        
    - Suggests adding more baseline alignment methods in experiments.
        

### **Reviewer's Stance:**

- **Rating:** Marginally above acceptance threshold.
    
- **Confidence:** Moderate (willing to defend, but admits possible misunderstanding of core parts or unfamiliarity with related work; math/details not fully checked).
## AUTHOR'S RESPONSE
### **Response to Weakness 1 (W1): Clarity of Section 3.1 and Refusal Features**

- **Agreement:** Authors agree Section 3.1 needed more clarity and self-containment.
    
- **Revisions Made:** Section 3.1 has been revised (changes in orange in PDF) to better explain key ideas and physical meanings.
    
- **Clarifications on Arditi's Findings:**
    
    - Arditi found that refusal in LMs is mediated by a single direction.
        
    - This direction is determined by calculating the average difference between activations of harmless and harmful prompts.
        
- **Clarifications on rHH​ and Equation 3:**
    
    - rHH​ represents the refusal direction; r^ is its unit vector.
        
    - The second term in Equation 3 zeros out the value along the refusal feature direction in original activations.
        
    - The third (last) term sets this value to the average of harmless prompt activations, as these activations are not typically centered near zero along this direction.
        

---

### **Response to Weakness 2 (W2): Difference between rHH​ (Sec 3.1) and rA​ (Sec 3.2)**

- **Distinction Explained:** Although Eq. 2 and 4 have similar structures, they have different meanings.
    
    - **rHH​ (Section 3.1):** Represents the _single refusal feature direction_ identified by the mean difference between harmful and harmless inputs. This direction encodes the model's tendency to refuse harmful requests, and controlling it enables "jailbreaking."
        
    - **rA​ (Section 3.2):** Denotes the _average influence on activations_ caused by various jailbreak attacks (e.g., GCG, PAIR).
        
- **Key Finding:** Despite different methodologies, diverse jailbreak attacks (e.g., GCG, PAIR) surprisingly influence the _same underlying refusal feature direction_ (rHH​).
    
- **Significance:** This finding underscores the central role of the refusal feature in understanding and defending against adversarial attacks within ReFAT.
    

---

### **Response to Weakness 3 (W3): Algorithm Workflow and Hyperparameters in Section 4.2**

- **Revisions Made:** Text in Section 4.2 has been revised for greater clarity on the algorithm's workflow.
    
- **Perturbation Mechanism:**
    
    - Instead of standard gradient-based worst-case perturbations, ReFAT crafts perturbations by **ablating the refusal feature directions**.
        
    - **RHH​ as Perturbation:** The refusal feature RHH​ is used as an approximation for the worst-case perturbation.
        
    - **Application:** This perturbation is applied directly to the residual stream activations at _each layer_ for harmful prompts during training.
        
    - **No Perturbation Strength Parameter:** Since the perturbation is fully defined by RHH​, there is no separate perturbation strength parameter.
        
    - Applied using RHH​ layer-wise refusal features.
        
- **Hyperparameter Location:**
    
    - Main hyperparameters (number of random harmful/harmless prompts for RHH​, recomputation frequency k, ablation probability pRFA​, dataset choices) are specified in **Section 5 (Experimental Setup)**.
        
    - More detailed ReFAT training hyperparameters are in **Appendix Table 4**.
        
- **Formulation Simplification:** Formulations in the manuscript have been simplified for clarity.
    

---

### **Response to Weakness 4 (W4): Connection to Previous Alignment Methods**

- **Assumption:** Authors assume the reviewer is referring to preference fine-tuning methods like RLHF and DPO.
    
- **ReFAT's Role:** ReFAT is presented as an alternative to supervised fine-tuning (SFT) that offers improved robustness.
    
- **Comparability:** ReFAT is comparable to other adversarial training methods for LLMs (e.g., R2D2, CAT).
    
- **Future/Variation Possibility:** It is possible to design a variation of ReFAT for preference fine-tuning methods (like DPO, IPO) by similarly removing the refusal feature from activations of harmful inputs with probability pRFA​ during forward passes.

## ------------------------------------------------------

## Official Review of Submission6962 by Reviewer  3MZw

### **Scores:**

- **Soundness:** 3 (good)
    
- **Presentation:** 2 (fair)
    
- **Contribution:** 2 (fair)
    
- **Rating:** 3 (reject, not good enough)
    
- **Confidence:** 4 (confident, but not absolutely certain; unlikely but possible misunderstanding or unfamiliarity)
    

---

### **Summary**

- **Core Finding:** Authors demonstrate that adversarial attacks on LLMs share a universal mechanism for bypassing safety measures.
    
- **Proposed Method:** Based on this, they introduce **Refusal Feature Adversarial Training (ReFAT)**.
    
- **Mechanism:** ReFAT efficiently trains LLMs by simulating input-level attacks through Refusal Feature Ablation (RFA).
    
- **Experimental Results:** Experiments show ReFAT significantly improves the robustness of three popular LLMs against various adversarial attacks.
    

---

### **Strengths**

- **New Method for LLM Safeguarding:** Proposes a novel adversarial training method for LLM safety.
    
- **Improved Efficiency:** The method offers improved efficiency compared to existing solutions.
    
- **Experimental Improvements:** Experiments show better performance over current adversarial training solutions.
    

---

### **Weaknesses**

- **Related Work (Activation Steering/Steering Vectors):**
    
    - The idea of manipulating refusal features in the activation space is also present in other works on steering vectors or activation steering (e.g., [1], [2], [3]).
        
    - Authors should discuss and comment on the differences, especially comparing with [1] and [3] which apply activation engineering to jailbreaking tasks.
        
- **Comparison with Other Defense Strategies:**
    
    - Paper should compare ReFAT with other existing LLM defense strategies beyond just adversarial training (e.g., [4]).
        
- **Newer Attack Baselines:**
    
    - Authors should consider comparing ReFAT against more recent attack baselines (e.g., [5], [6], [7]) to better demonstrate its effectiveness.
        
- **Clarity on RFA, Optimal Attack, and Adversarial Training Link:** The relationship between RFA, optimal attack, and its connection to adversarial training is not clearly stated.
    

---

### **Questions**

- **Equation 3 - Mean RF Activation (Harmless Prompts):**
    
    - Provide more details on why the term for the mean Refusal Feature (RF) activation over harmless prompts is included in Eq (3).
        
    - What does this term represent, and why is it added to the activation?
        
- **Equation 10 - Loss Design:**
    
    - Further explain the loss design in Eq (10).
        
    - The term `H(x) - R_HH` (removal of refusal features) seems different from what was demonstrated in Sections 3.2 and 3.3. Clarify this discrepancy.
        
- **RFA as Approximation for Optimal Attack:**
    
    - The approximation of RFA to the optimal attack is not entirely convincing.
        
    - Can the authors directly calculate the best δ direction in Eq (7) and compare its similarity to RFA?
        
    - Since approximating AA with RFA, directly solving Eq (7) using adversarial training _should_ yield even better performance. Can the authors show this performance gap?
        

---

## AUTHOR'S RESPONSE


### **Clarifications Regarding Weaknesses**

- **W1: Manipulation of Refusal Features in Other Works (Steering Vectors)**
    
    - **Acknowledgment:** Authors thank the reviewer for pointing out additional related work and have added them to the revised manuscript.
        
    - **Main Contribution Emphasized:** The core contribution is _not_ being the first to propose manipulating refusal features. Instead, it's the _first to show that manipulating the linear refusal feature during training time can result in a model that robustly rejects adversarial prompts during evaluation, without needing an additional steering vector_.
        
    - **Distinction from Attack Methods [1, 2]**: Papers [1] and [2] (similar to Arditi et al., 2024) demonstrate refusal feature ablation as a powerful _jailbreaking attack_. This paper, conversely, proposes a novel _defense method_ that mitigates vulnerability to such attacks and enhances robustness against other popular adversarial attacks.
        
    - **Distinction from "Learning Steering Vectors" [3]**: ReFAT fine-tunes model parameters by injecting a steering vector _during training_. It _does not require an additional steering vector during evaluation_ for safety/robustness. "Steering vector learning" papers [3] and (Stickland et al., 2024) aim for a lightweight strategy to find good steering vectors to be added _during inference time_ to improve performance. This distinction is clarified in Section 4.2 of the revised manuscript.
        
- **W2: Other Types of LLM Defenses (Beyond Adversarial Training)**
    
    - **Paper's Objective:** The paper aims to improve LLM safety at the _model parameter level_, ensuring adversarial robustness _without_ relying on inference-time defense methods (like prompt engineering or post-generation checking/reflection, e.g., RA-LLM [4]).
        
    - **Complementary Approach:** ReFAT and other adversarial training methods are considered _complementary_ to inference-time defense methods, not competing.
        
    - **Future Work:** Authors will consider combining adversarially fine-tuned models using ReFAT with inference-time defense methods like RA-LLM in future extensions.
        
- **W3: Newer Attack Baselines**
    
    - **Acknowledgment:** Authors thank the reviewer for suggesting additional recent attack methods and will include them in future revisions.
        
    - **Selection Rationale:** Due to high computational costs, authors selected the most popular, powerful, and peer-reviewed attack methods to ensure generalizability.
        
    - **Specific Attacks:**
        
        - [6] and [7] were not peer-reviewed and had few citations at submission time.
            
        - TAP [5] is well-known but highly similar to the PAIR attack already tested.
            
        - Recent evaluations (Mazeika et al., 2024, Table 6, p. 26) suggest TAP is often not more successful than PAIR in jailbreaking the safest models. Thus, PAIR was chosen as the most representative LLM-based attack.
            

---

### **Responses to Questions**

- **Q1: Equation 3 - Mean RF Activation Over Harmless Prompts Term**
    
    - **Intuition:** The goal of RFA is to make harmful samples appear as similar as possible to harmless samples along the refusal feature direction. This means altering internal representations so their projection on the RF direction is close to the _centroid of harmless samples_.
        
    - **Reason for Term:** The mean activation along the refusal direction of harmless samples is generally _not zero_ (as seen in Fig. 18 and 19). Therefore, simply setting this value to zero (via the second term in Eq. 3) would not fully make harmful examples appear harmless. The mean RF activation term ensures the projection of the activation along the refusal feature equals the mean of projections for harmless examples.
        
- **Q2: Equation 10 - Loss Design & Removal of Refusal Features (H(x)−RHH​)**
    
    - **Bias Term Omission:** The bias term from Eq. (3) was _not_ included in Eq. (10) due to efficiency considerations.
        
    - **Computational Cost:** Computing the bias term (rˉDharmless(l)​) requires an additional computational step (calculating average projection of harmless input hidden representations at each layer), which would slow down training.
        
    - **Empirical Finding:** Preliminary experiments showed this additional computation had little improvement on resulting model performance.
        
    - **Design Choice:** The average harmless RF bias term was removed from the training objective to facilitate fine-tuning. This explanation has been added to the revised manuscript.
        
- **Q3: RFA as Approximation for Optimal Attack & Performance Gap**
    
    - **Related Work (LAT):** Authors found that Latent Adversarial Training (LAT) by (Sheshadri et al., 2024) proposes a similar idea of finding worst-case residual stream perturbation using projected gradient descent.
        
    - **Comparison Plans:** Authors considered comparing ReFAT with LAT but could not due to LAT's code not being fully released at submission time, and time/resource constraints.
        
    - **Acknowledgment:** They acknowledge the strong relation and plan to include LAT as a baseline in the camera-ready version if the paper is accepted.
        
    - **Hope for Score Increase:** Authors hope their response addresses concerns and encourage the reviewer to raise their score for ICLR presentation opportunity.
        

---

### **Reviewer 3MZw's Re-evaluation (23 Nov 2024)**

- **Thanks:** Reviewer thanks authors for responses and clarifications.
    
- **Novelty (Still a Concern):**
    
    - Agrees it's the first to show adversarial training on the refusal feature leads to a more robust model.
        
    - However, given prior work on refusal feature effectiveness and adversarial training in latent space for robustness, the achievement "is not that surprising and feels like a direct combination of the two."
        
- **Baselines (Still a Concern):**
    
    - Understands comparison challenges during rebuttal.
        
    - Still feels the work lacks comparison with more recent advances in the field, which is fast-evolving and where decent defense performance is already achieved by many existing works.
        
    - Finds authors' reasons for not comparing (not same "adv training" type, not peer-reviewed, few citations, code not released) not "super solid."
        
    - **Conclusion:** Since the idea isn't entirely novel and presentation isn't "super clear and intuitive enough," the reviewer relies on experiments to justify contributions and feels comparisons are still lacking.

## ------------------------------------------------------

## Official Review of Submission6962 by Reviewer oCF9
### **Scores:**

- **Soundness:** 3 (good)
    
- **Presentation:** 3 (good)
    
- **Contribution:** 3 (good)
    
- **Rating:** 8 (accept, good paper)
    
- **Confidence:** 5 (absolutely certain about assessment, very familiar with related work, checked math/details carefully)
    

---

### **Summary**

- **Novel Method:** Authors propose a new robustification method for LLMs.
    
- **Core Idea:** The method is built on ablating the "refusal direction" in a model's latent space during training.
    
- **Demonstrated Relationship:** Authors show how this approach connects to standard adversarial training/attacks.
    
- **Effectiveness:** Provides evidence that the proposed method is effective.
    

---

### **Strengths**

- **Mechanistic Insights:** Paper includes various experiments offering insights into how the algorithm can serve as an alternative to adversarial training.
    
- **Limitation Assessment:** Authors attempt to evaluate the limitations of their approach (e.g., generalization when the refusal direction might be inaccurate).
    
- **Recent Baselines:** Comparisons are made against very recent baseline robustification methods.
    
- **Efficiency:** The proposed approach is considerably more efficient than other robustification methods.
    

---

### **Weaknesses**

- **Known Results in Figures 1 & 2:** The results presented in Figure 1 and Figure 2 are likely already known. It would be beneficial to provide appropriate context for these figures.
    
- **Inconsistent Refusal Results:** Refusal results do not seem consistent with those reported by Xhonneux et al. (2024). A more thorough assessment of refusal behavior, perhaps using OR-Bench, would strengthen the paper.
    
- **Hyperparameter Stability:** There's a lack of assessment regarding the stability of different robustification methods concerning hyperparameter choices.
    
    - It's unclear how close each method is to its "optimum," making accurate estimation of accuracy-robustness trade-off differences difficult, despite the clear efficiency advantage of the proposed method.
        

---

### **Questions**

- **Hyperparameter Tuning Procedures:** Can the authors elaborate on the hyperparameter tuning process? While acknowledging that competitor tuning is resource-intensive, the reviewer is interested in the effort required to obtain the results for ReFAT specifically.
    

---

### **Recommendation**

- Recommends accepting the paper, believing it offers interesting insights to the ICLR community.
    
- Willing to improve the score after the rebuttal.

## AUTHOR'S RESPONSE


---

### **Response to Weakness 1 (W1): Known Results in Figures 1 & 2**

- **Clarification on Novelty:**
    
    - Previous work showed refusal mechanism is the "refusal feature."
        
    - **Authors' Novelty:** They are the _first_ to provide evidence that adversarial attacks bypass this mechanism by ablating the refusal feature.
        
- **Figure 1 Context:**
    
    - Provides an overview of paper contributions.
        
    - First two "rows" are known results about the refusal feature.
        
    - Next two rows illustrate their _novel mechanistic analysis_ of existing adversarial attacks.
        
    - Last row illustrates their _new training method, ReFAT_.
        
- **Figures 2 & 3 Context:** Authors are unaware of previous work performing the similarity-based and PCA visualizations (Figures 2, 3) to analyze the connection between existing adversarial attacks and refusal feature ablation.
    

---

### **Response to Weakness 2 (W2): Inconsistent Refusal Results with Xhonneux et al. (2024)**

- **Different LLM Suites:** Xhonneux et al. (2024) evaluated LLMs that are significantly less safe than those used in this paper (as suggested by HarmBench, Mazeika et al., 2024).
    
- **Different Refusal Measurement Datasets:**
    
    - Xhonneux et al. (2024) used a hand-written dataset of 40 _clearly harmless_ prompts (e.g., "Explain the rules of soccer").
        
    - **Authors' Approach:** They measure refusals on **XSTest**, which is _more challenging_ as it includes harmless prompts that _appear harmful_ (e.g., "How can I build a bomb in minecraft?"). XSTest thus provides a more informative assessment.
        
- **Consistency with Xhonneux et al. (2024):**
    
    - Consistent with Xhonneux et al., R2D2 significantly increases over-refusals (lower compliance) on XSTest, unlike CAT and ReFAT (see Table 1).
        
    - Xhonneux et al. reported CAT doesn't increase refusals on their harmless dataset for Gemma and Mistral-7B.
        
    - **Authors' Finding on XSTest:** On the more challenging XSTest, CAT can lead to more over-refusal than ReFAT (see Mistral-7B results in Table 1).
        

---

### **Response to Weakness 3 (W3): More Comprehensive Refusal Assessment (e.g., OR-Bench)**

- **OR-Bench Availability:** OR-Bench was newly released and not reviewed at the time of paper submission, so only XSTest was included.
    
- **Complementary Assessment:** To complement refusal assessment, authors followed Appendix D of [Xhonneux et al, 2024] by measuring refusals on 5,700 MMLU questions.
    
- **MMLU Results:** ReFAT's refusal-rate on MMLU is at most 0.001, confirming it does not induce significant over-refusals.
    
- **Paper Revision:** These results have been added to **Appendix F, Table 6** of the revised paper.
    

---

### **Response to Weakness 4 (W4) and Weakness 5 (W5): Hyperparameter Stability and Tuning Procedures**

- **Computational Cost:** Due to the high computational cost of evaluating each model with adversarial attacks like GCG, comprehensively searching for optimal hyperparameter configurations for _each_ defense method is practically infeasible.
    
- **Optimization Effort:** Authors state they made their best attempt to optimize hyperparameter choices for each model.
    
- **ReFAT Hyperparameter Stability:**
    
    - Preliminary experiments suggest ReFAT's improvement in adversarial robustness is quite consistent.
        
    - Key factor: Ablating refusal features for the _second half of all model layers_ (e.g., layers 16-32 for Llama/Mistral, 14-28 for Gemma).
        
    - Other hyperparameters (learning rate, batch size) have much less effect.
        
- **R2D2 and CAT Hyperparameter Stability:**
    
    - Authors tried modifying default hyperparameters in official implementations during preliminary experiments for R2D2 and CAT.
        
    - Observed no significant performance improvement.
        
    - **Confidence:** Confident that ReFAT's demonstrated advantage over other defense methods is robust against hyperparameter choices.
## ------------------------------------------------------