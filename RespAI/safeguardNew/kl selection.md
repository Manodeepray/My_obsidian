Short answer: for unlearning, use **bothâ€”but in different places**.

- **On the forget set (the stuff you want to erase):** push the model **away** with a **forward-KL style, anti-likelihood** term. Concretely, _minimize_  
    Lforget=E(x,y)âˆ¼Df[logâ¡qÎ¸(yâˆ£x)]L_{\text{forget}} = \mathbb{E}_{(x,y)\sim \mathcal{D}_f}[\log q_\theta(y|x)]  
    (equivalently, _maximize_ the forward KL DKL(pfâˆ¥qÎ¸)D_{\mathrm{KL}}(p_f\|q_\theta)).  
    This gives a clean, per-example gradient that directly **lowers** the modelâ€™s probability on the forgotten samples/tokens. Reverse KL against an empirical forget distribution is ill-posed and not what you want here.
    
- **To keep the modelâ€™s overall behavior intact (retain/safety constraint):** anchor the new model to the base/reference with **reverse KL**  
    Lanchor=Ex[DKL(qÎ¸(â‹…âˆ£x)â€‰âˆ¥â€‰Ï€0(â‹…âˆ£x))]L_{\text{anchor}}=\mathbb{E}_{x} \big[D_{\mathrm{KL}}(q_\theta(\cdot|x)\,\|\,\pi_0(\cdot|x))\big].  
    Reverse KL is **mode-seeking/model-seeking**: it strongly penalizes assigning probability where the reference assigns little, so it prevents the model from â€œleakingâ€ probability into odd or unsafe regions while youâ€™re pushing away from the forget set. Forward KL to the reference would try to cover _all_ reference modes (including potentially bad ones) and is more permissive about drifting into new regionsâ€”undesirable for unlearning.
    

A practical composite objective (token-level or sequence-level) looks like:

minâ¡Î¸â€…â€ŠE(x,y)âˆ¼Drâ€‰â£[âˆ’logâ¡qÎ¸(yâˆ£x)]âŸtrainÂ onÂ retainâ€…â€Š+â€…â€ŠÎ»â€‰DKLâ€‰â£(qÎ¸(â‹…âˆ£x)â€‰âˆ¥â€‰Ï€0(â‹…âˆ£x))âŸreverse-KLÂ anchorâ€…â€Š+â€…â€ŠÎ¼â€‰E(x,y)âˆ¼Dfâ€‰â£[logâ¡qÎ¸(yâˆ£x)]âŸforward-KLÂ push-away\min_\theta\; \underbrace{\mathbb{E}_{(x,y)\sim \mathcal{D}_r}\!\big[-\log q_\theta(y|x)\big]}_{\text{train on retain}} \;+\;\lambda\,\underbrace{D_{\mathrm{KL}}\!\big(q_\theta(\cdot|x)\,\|\,\pi_0(\cdot|x)\big)}_{\text{reverse-KL anchor}} \;+\;\mu\,\underbrace{\mathbb{E}_{(x,y)\sim \mathcal{D}_f}\![\log q_\theta(y|x)]}_{\text{forward-KL push-away}}

with Î»>0,Î¼>0\lambda>0,\mu>0. Intuition:

- the **retain term** preserves utility,
    
- the **reverse-KL anchor** keeps you close to the original distribution (prevents collateral damage),
    
- the **anti-likelihood (forward-KL) term on the forget set** actively removes the undesired behavior.
    

If you must pick **one** divergence for an explicit target policy pgoodp_{\text{good}} (e.g., a masked/renormalized distribution that zeroes forbidden outputs), then minimizing **reverse KL** DKL(qâˆ¥pgood)D_{\mathrm{KL}}(q\|p_{\text{good}}) is safest: it puts (near-infinite) pressure against allocating mass to forbidden tokens.

Tuning tip:

- Increase **Î¼\mu** until the modelâ€™s probability on forget examples is driven low (check token-level log-probs/perplexity on Df\mathcal{D}_f),
    
- Increase **Î»\lambda** to recover general performance on retain tasks and to curb unintended drift.
    

So: **forward-KL (as an anti-likelihood) to forget; reverse-KL to the reference to stabilize.**

---



Good â€œwhat ifâ€! Letâ€™s reason carefully about that swap.

---

### 1. **If you unlearn with reverse KL**

That means:

$$Lforget=DKL(qÎ¸(â‹…âˆ£x)â€‰âˆ¥â€‰pf(â‹…âˆ£x))L_\text{forget} = D_{\mathrm{KL}}(q_\theta(\cdot|x)\,\|\,p_f(\cdot|x))
$$
where pfp_f is the empirical forget distribution (e.g., delta at the ground-truth label).  
But pfp_f is _very peaky_ (only mass on the forbidden label).

- Minimizing this reverse KL forces **mode-seeking toward the forbidden label**â€”the _opposite_ of what you want.
    
- If you instead try to _maximize_ it, the gradients blow up and are unstable (reverse KL is undefined when pf=0p_f=0 for outcomes that qÎ¸q_\theta assigns mass to).
    
- In practice, reverse KL is a bad fit for â€œpush-awayâ€ because it doesnâ€™t gracefully reduce probabilityâ€”it either collapses or diverges.
    

---

### 2. **If you learn with forward KL**

That means aligning with retain/reference data using:

$$Lretain=DKL(pr(â‹…âˆ£x)â€‰âˆ¥â€‰qÎ¸(â‹…âˆ£x))L_\text{retain} = D_{\mathrm{KL}}(p_r(\cdot|x)\,\|\,q_\theta(\cdot|x))$$

where prp_r is the empirical distribution on retained data.  
Forward KL is **mean-seeking**: it makes the model spread probability to cover all modes in the retain distribution.

- This works fine for supervised MLE (which is exactly minimizing forward KL).
    
- But in unlearning, this encourages the model to **over-cover** rather than stay close to the _original distribution_.
    
- So utility may be preserved on average, but youâ€™ll get more drift/instability in non-retained regions compared to reverse KL anchoring.
    

---

### 3. **Comparison of your swap vs the â€œusual recipeâ€**

- **Standard practice (better):**
    
    - Unlearning: forward-KL/anti-likelihood â†’ controlled push-away.
        
    - Retain: reverse-KL â†’ stable anchoring to reference.
        
- **Your swap (reverse unlearn + forward retain):**
    
    - Unlearning: reverse KL â†’ unstable, misaligned with the â€œeraseâ€ goal.
        
    - Retain: forward KL â†’ okay, but weaker at constraining drift.
        

So, your setup is both **less effective at forgetting** and **less effective at preventing collateral damage**.

---

âœ… **Bottom line:**

- Using **reverse KL for unlearning** is problematic, because itâ€™s mode-seeking and doesnâ€™t naturally reduce mass on forget samples.
    
- Using **forward KL for retaining** is serviceable, but not as strong as reverse KL anchoring for stability.
    

ğŸ‘‰ The asymmetry matters: **forgetting needs forward KL (anti-likelihood)**; **retaining needs reverse KL (anchoring)**.

---

Do you want me to sketch the **gradient behavior** of forward vs reverse KL on a toy binary classification example? That makes it crystal-clear why reverse KL fails for unlearning.




---


This is a really sharp question â€” and I see where the confusion comes in. Letâ€™s check the passage against what we know about **forward vs reverse KL** behavior:

---

### 1. **Forward KL in the alignment/regularization phase**

They wrote:

> _Forward KL (Ï€_ref || Ï€_Î¸) is mean-seeking / zero-avoiding; ensures Ï€_Î¸ covers all of Ï€_refâ€™s support, preventing catastrophic forgetting._

âœ… This part is **correct**.

- Minimizing DKL($$Ï€refâ€‰âˆ¥â€‰Ï€Î¸)D_{KL}(\pi_{ref}\,\|\,\pi_\theta$$) is exactly MLE / standard distillation.
    
- It makes the student cover all modes of the reference distribution â†’ â€œmean-seekingâ€ / â€œzero-avoiding.â€
    
- Using it for **regularization** to avoid forgetting is exactly how RLHF/DPO papers often frame it.
    

---

### 2. **Reverse KL for aggressive refusal**

They wrote:

> _Reverse KL (Ï€_harmful || Ï€_Î¸) is mode-seeking / zero-forcing, and punishes Ï€_Î¸ for putting mass anywhere Ï€_harmful doesnâ€™t._

âš ï¸ This part is **not correct in the way itâ€™s written** â€” because of the ordering of arguments.

- If you write DKL($$Ï€harmfulâ€‰âˆ¥â€‰Ï€Î¸)D_{KL}(\pi_{harmful}\,\|\,\pi_\theta$$):
    
    $$Eyâˆ¼Ï€harmful[logâ¡Ï€harmful(y)Ï€Î¸(y)]\mathbb{E}_{y \sim \pi_{harmful}} \Big[\log \frac{\pi_{harmful}(y)}{\pi_\theta(y)}\Big]$$
    
    This expects you to _sample from the harmful distribution_.  
    Minimizing it encourages Ï€Î¸ to **imitate Ï€_harmful** (not refuse it!) since the KL is minimized when Ï€Î¸ = Ï€_harmful.
    
- What you actually want for refusal is something like:
    
    DKL($$Ï€Î¸â€‰âˆ¥â€‰Ï€safe)D_{KL}(\pi_\theta \,\|\, \pi_{safe}$$)
    
    where Ï€_safe zeroes out (or renormalizes away) harmful tokens. Reverse KL with Ï€Î¸ in the numerator is â€œzero-forcingâ€ â€” i.e. if $$Ï€_{safe}(y) = 0, then KL â†’ âˆ,  unless ,Ï€Î¸(y)=0.$$
    

So the **concept (â€œmode-seeking, zero-forcing refusalâ€) is right**, but the **notation/order is flipped**.  
It should be$$ DKL(Ï€Î¸â€‰âˆ¥â€‰Ï€harmful)D_{KL}(\pi_\theta \,\|\, \pi_{harmful}) or DKL(Ï€Î¸â€‰âˆ¥â€‰Ï€safe)D_{KL}(\pi_\theta \,\|\, \pi_{safe}),$$

not

$$ DKL(Ï€harmfulâ€‰âˆ¥â€‰Ï€Î¸)D_{KL}(\pi_{harmful} \,\|\, \pi_\theta).
$$
---

### âœ… Corrected framing:

- **Forward KL (Ï€_ref || Ï€_Î¸):** alignment regularization â†’ mean-seeking, prevents forgetting.
    
- **Reverse KL (Ï€_Î¸ || Ï€_safe):** unlearning / refusal â†’ zero-forcing, drives Ï€_Î¸â€™s probability on harmful responses to _exactly zero_.
    

---

ğŸ‘‰ So: the _intuition_ in your quoted context is fine, but the **argument order in the reverse KL term is off** â€” if used literally as written, it would actually push the model _toward_ the harmful policy.

---

Do you want me to show you a **tiny binary example** (harmful vs safe token) where the gradients flip sign depending on whether you do forward or reverse KL, so you can see exactly why the argument order matters?