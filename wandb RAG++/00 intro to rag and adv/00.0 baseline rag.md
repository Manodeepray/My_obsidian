**TLDR:** In this chapter, we built a simple RAG system using W&B documentation as our knowledge base. We learned about data pre-processing, chunking, vectorization, retrieval, response generation, and the importance of tools like Weights & Biases Weave for tracking and evaluating LLM applications. In the next chapter, we'll build upon these concepts to create more advanced RAG systems.

**Links:** 

- Course chapter [notebook](https://github.com/wandb/edu/blob/main/rag-advanced/notebooks/Chapter01.ipynb)
- Weights & Biases Weave [Documentation](https://weave-docs.wandb.ai/?utm_source=course&utm_medium=course&utm_campaign=rag_course) 
- Wandbot [repo](https://github.com/wandb/wandbot)
- LLM-friendly [resources](https://github.com/wandb/edu/tree/main/rag-advanced/resources) 
- [Cohere Tokenizer](https://docs.cohere.com/docs/tokens-and-tokenizers?utm_source=course&utm_medium=course&utm_campaign=rag_course)

  

In this hands-on session, we'll build a basic RAG application using the Weights & Biases (W&B) [documentation](https://docs.wandb.ai/?utm_source=course&utm_medium=course&utm_campaign=rag_course) as our knowledge base. Let's walk through the steps involved:

**1. Initial Setup and Data Download:**

We'll start by setting up the necessary packages and downloading the W&B documentation dataset using a W&B artifact.

**2. Data Preprocessing:**

- Converting to Dictionaries: We'll convert the Markdown documents into a list of dictionaries, where each dictionary represents a document and contains the document's content and metadata.
- Metadata: We'll include metadata like the source of the document and the number of tokens it contains.

**3. Introducing Weights & Biases Weave:**

Weights & Biases [Weave](https://weave-docs.wandb.ai/?utm_source=course&utm_medium=course&utm_campaign=rag_course) is a powerful toolkit for tracking and evaluating LLM applications. It helps us:

- Log and debug LLM inputs and outputs.
- Trace LLM calls and understand the flow of information.
- Version datasets and models for better reproducibility and tracking.

**4. Creating a Weave Dataset:**

We'll use Weave to create a dataset from our raw data and publish it for versioning and tracking.

**5. Chunking the Data:**

Why Chunk? Chunking is important because most embedding models have token limits (e.g., 512 tokens). It also helps reduce the amount of text sent to the LLM, improving efficiency.

We'll use a basic chunking method that divides the text into chunks of approximately 500 tokens without any overlap.

**6. Making Text Tokenization Safe:**

Cleaning Special Tokens: We need to remove or replace special tokens that might interfere with the LLM's tokenization process. These tokens (like "start of text" or "end of text") are specific to each LLM.

[Cohere Tokenizer](https://docs.cohere.com/docs/tokens-and-tokenizers?utm_source=course&utm_medium=course&utm_campaign=rag_course): We'll use a cleaning process based on the Cohere tokenizer because we'll be using the Cohere Command model for response generation.

**7. Storing Chunk Data in a Weave Artifact:**

We'll store the cleaned, chunked data in a Weave artifact for easy access and versioning.

**8. Vectorizing the Data:**

Why Vectorize? Vectorization converts text into numerical representations (vectors), allowing us to perform similarity searches.

We'll use a simple TF-IDF vectorizer to create vectors for our chunks. It essentially creates vectors based on the frequency of words in each chunk compared to their frequency in the entire dataset.

**9. Creating a Weave Model (TF-IDF Retriever):**

We'll create a Weave model to encapsulate our TF-IDF retriever. This model will:

- Index Data: Take the document chunks and create an index using the TF-IDF vectorizer.
- Search: Take a user query, convert it into a vector, calculate the cosine distance between the query vector and the document vectors in the index, and retrieve the top k most similar documents.

**10. Testing the Retriever:**

We can test our TF-IDF retriever with a simple query and see which documents it retrieves.

**11. Creating a Response Generator (Weave Model):**

We'll create another [Weave model](https://weave-docs.wandb.ai/guides/core-types/models/?utm_source=course&utm_medium=course&utm_campaign=rag_course) for our response generator. This model will:

- Take Input: Receive the user query and the retrieved context from the retriever.
- Use an Instruction: Use a carefully crafted instruction to guide the LLM's response generation. The instruction will tell the LLM to answer the question based only on the provided context.
- Generate Response: Use the Cohere Command model to generate the final response.

**12. Building the RAG Pipeline:**

We'll combine the retriever and the response generator into a single RAG pipeline. This pipeline will:

- Take a user query as input.
- Retrieve relevant documents using the retriever.
- Generate a response using the response generator.

**13. Exploring the Weave Dashboard:**

**![](https://files.cdn.thinkific.com/file_uploads/705742/images/8b6/614/693/1726828888856.png)**  

The Weave dashboard allows us to visualize the entire RAG pipeline and the flow of information. We can see the nested calls to the retriever and the response generator, along with their inputs and outputs.