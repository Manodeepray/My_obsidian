**TLDR:** Building a robust evaluation process involves creating a meaningful evaluation set, leveraging tools like Weave, and understanding the different approaches to evaluating RAG components, such as the retriever and response generator, to ensure the system meets your specific needs and use case

**Links:** 

- Chapter [notebook](https://github.com/wandb/edu/blob/main/rag-advanced/notebooks/Chapter02.ipynb) 
- Weights & Biases Weave [Documentation](https://weave-docs.wandb.ai/?utm_source=course&utm_medium=course&utm_campaign=rag_course) 
- Wandbot [repo](https://github.com/wandb/wandbot)
- LLM-friendly [resources](https://github.com/wandb/edu/tree/main/rag-advanced/resources)
- How to Evaluate an LLM
    - [Part 1: Building an Evaluation Dataset for our LLM System](https://wandb.ai/wandbot/wandbot-eval/reports/How-to-Evaluate-an-LLM-Part-1-Building-an-Evaluation-Dataset-for-our-LLM-System--Vmlldzo1NTAwNTcy?utm_source=course&utm_medium=course&utm_campaign=rag_course)
    - [Part 2: Manual Evaluation of Wandbot, our LLM-Powered Docs Assistant  
        ](https://wandb.ai/wandbot/wandbot-eval/reports/How-to-Evaluate-an-LLM-Part-2-Manual-Evaluation-of-Wandbot-our-LLM-Powered-Docs-Assistant--Vmlldzo1NzU4NTM3?utm_source=course&utm_medium=course&utm_campaign=rag_course)
    - [Part 3: LLMs evaluating LLMs](https://wandb.ai/wandbot/wandbot-eval/reports/How-to-evaluate-an-LLM-Part-3-LLMs-evaluating-LLMs--Vmlldzo1NzEzMDcz?utm_source=course&utm_medium=course&utm_campaign=rag_course)

  

In this chapter's [notebook](https://github.com/wandb/edu/blob/main/rag-advanced/notebooks/Chapter02.ipynb), we'll dive into the practical aspects of evaluating a RAG system's components – specifically, the retriever and the response generator.

**Weave Evaluation:**

We'll leverage Weave, to assess the performance of the RAG system. It provides a convenient way to track evaluation scores and the traces (the retrieval and generation process) for each query.

**Building the Evaluation Set:**

The first step in any evaluation is to create a suitable evaluation dataset. For this chapter, we'll use a subset of the evaluation data we created for Wandbot.

Wandbot Evaluation Set: 

- Initial Eyeballing: We initially deployed Wandbot based on rigorous eyeballing – meaning we manually reviewed a lot of responses to get a sense of its performance.
- Query Distribution Analysis: To build a more formal evaluation set, we analyzed the distribution of user queries to understand the different types of questions Wandbot would encounter.
- Gold Standard Set: We then sampled queries from different clusters in the query distribution to create a "gold standard" set of questions and their corresponding ground truth answers and contexts.
- Manual Evaluation with [Argilla](https://docs.argilla.io/latest/?utm_source=course&utm_medium=course&utm_campaign=rag_course): We used in-house Machine Learning Engineers (MLEs) as domain experts to perform manual evaluation of Wandbot's responses using a tool called Argilla. This allowed us to get detailed, human-judged assessments of the system's performance.

Important Note: Building evaluation platforms and tools is much easier these days. The key takeaway is to prioritize speed when building your initial evaluation set. Use whatever tools and resources you have available to create a meaningful eval set that is representative of your use case.

**Weave Dataset:**

The evaluation samples are stored as a [Weave Dataset](https://weave-docs.wandb.ai/guides/core-types/datasets?utm_source=course&utm_medium=course&utm_campaign=rag_course), which provides a structured way to organize the questions, ground truth answers, and associated contexts. You can easily explore the dataset to examine individual examples and understand how the system is performing.

**Versioning Evaluation Sets:**

As you experiment and refine your RAG system, you'll likely create multiple versions of your evaluation set. It's essential to keep track of these versions so you can monitor progress and understand how changes to your system affect its performance over time. Weave provides tools to manage and track different versions of your evaluation dataset..