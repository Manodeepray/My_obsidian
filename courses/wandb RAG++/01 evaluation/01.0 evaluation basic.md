**TLDR:** Evaluating RAG systems involves assessing both the overall system performance and the quality of individual components using a variety of methods, datasets, and metrics tailored to your specific application and use case.

**Links:** 

- Chapter [notebook](https://github.com/wandb/edu/blob/main/rag-advanced/notebooks/Chapter02.ipynb) 
- Weights & Biases Weave [Documentation](https://weave-docs.wandb.ai/?utm_source=course&utm_medium=course&utm_campaign=rag_course) 
- Wandbot [repo](https://github.com/wandb/wandbot)
- LLM-friendly [resources](https://github.com/wandb/edu/tree/main/rag-advanced/resources)
- [Evaluation-Driven Development: Improving WandBot, our LLM-Powered Documentation App](https://wandb.ai/wandbot/wandbot_public/reports/Evaluation-Driven-Development-Improving-WandBot-our-LLM-Powered-Documentation-App--Vmlldzo2NTY1MDI0?utm_source=course&utm_medium=course&utm_campaign=rag_course)
- [Refactoring Wandbot—our LLM-powered document assistant—for improved efficiency and speed](https://wandb.ai/wandbot/wandbot_public/reports/Refactoring-Wandbot-our-LLM-powered-document-assistant-for-improved-efficiency-and-speed--Vmlldzo3NzgyMzY4?utm_source=course&utm_medium=course&utm_campaign=rag_course)

  

Evaluating a RAG system involves assessing not just the overall performance but also the quality of its individual components.

**Types of Evaluation:**

**End-to-End (System) Evaluation:**

We start by evaluating the final response generated by the entire RAG pipeline. This is important because the response is often non-deterministic – meaning it can vary even for the same input query. We typically compare the generated response against a "ground truth" or ideal response to assess its quality.

**Component Evaluation:**

Since RAG systems have multiple parts (retrieval, re-ranking, generation), we also need to evaluate these components individually. This helps us pinpoint areas for improvement. For example, we might evaluate the retrieval system by comparing the retrieved context against a ground truth context or by asking an LLM to judge if the generated response is actually based on the retrieved context.

**Evaluation Without Ground Truth:**

Not all evaluations require ground truth. Here are some cases where we can evaluate without it:

- Direct Evaluation: We can measure specific aspects of a response directly. For example, we can use tools to assess the toxicity or racial bias of a generated response, or we can check if the response is grounded in the source text (e.g., by looking for citations).
- Pairwise Evaluation: We can compare two or more responses generated for the same query and judge which one is better based on criteria like tone, coherence, or informativeness.

**Evaluation with Ground Truth (Reference Evaluation):**

When we have a gold standard reference (a "correct" answer), we can perform reference evaluation. This is often used to evaluate the structure of a response or to check if it includes specific information that is expected.

**Evaluation Methods in Practice:**

- Eyeballing: The quickest way to evaluate is to simply look at the responses and see if they seem reasonable. However, this is not very reliable and won't catch all problems, especially edge cases.
- Manual Evaluation: Hiring human annotators (experts in the relevant domain) to evaluate responses is expensive and time-consuming, but it's the most reliable method.
- LLM as a Judge: We can use a powerful LLM to automatically score responses or evaluate components of our system. This is becoming increasingly popular as LLMs get better at understanding and evaluating language.

**Evaluation Datasets and Metrics:**

The effectiveness of your evaluation depends on your dataset and the metrics you choose.

- Relevance to Production: Your evaluation dataset should closely resemble the real-world data your system will encounter (the "production distribution"). Similarly, your evaluation metrics should be relevant to your specific use case.
- Challenges with Public Benchmarks: Public benchmarks can be helpful for comparing different systems, but they often don't perfectly match your specific production distribution or use case.
- Human Evaluation and User Testing: While slower and more expensive, human evaluation and user testing provide the most direct and relevant assessments of system performance.

**The Ideal Evaluation Approach:**

The goal is to build a small but highly representative evaluation dataset and leverage LLM judges to evaluate different components of your RAG system. By using techniques like LLM alignment (training LLMs to be better judges), we can push this mode of evaluation towards being both highly correlated with real-world performance and efficient enough for rapid iteration cycles.

  

**Links:** 

- Chapter [notebook](https://github.com/wandb/edu/blob/main/rag-advanced/notebooks/Chapter02.ipynb) 
- Weights & Biases Weave [Documentation](https://weave-docs.wandb.ai/?utm_source=course&utm_medium=course&utm_campaign=rag_course) 
- Wandbot [repo](https://github.com/wandb/wandbot)
- [Evaluation-Driven Development: Improving WandBot, our LLM-Powered Documentation App](https://wandb.ai/wandbot/wandbot_public/reports/Evaluation-Driven-Development-Improving-WandBot-our-LLM-Powered-Documentation-App--Vmlldzo2NTY1MDI0?utm_source=course&utm_medium=course&utm_campaign=rag_course)
- [Refactoring Wandbot—our LLM-powered document assistant—for improved efficiency and speed](https://wandb.ai/wandbot/wandbot_public/reports/Refactoring-Wandbot-our-LLM-powered-document-assistant-for-improved-efficiency-and-speed--Vmlldzo3NzgyMzY4?utm_source=course&utm_medium=course&utm_campaign=rag_course)

  

**Key Takeaway:** Evaluating RAG systems involves assessing both the overall system performance and the quality of individual components using a variety of methods, datasets, and metrics tailored to your specific application and use case.