# Group Relative Policy Optimization



# DeepSeek-R1 Dissection: Understanding PPO & GRPO Without Any Prior Reinforcement Learning Knowledge
[Article](https://huggingface.co/blog/NormalUhr/grpo)

Analogy : RL training process as an elementary school exam scenario  
We ( the model being trained ) are like students trying to get high grades , the teacher who grades our exams are like the reward model ,
while our parents handling out pocket money based on or grades is similar to the critic .

- final scores alone are insufficient ,
	- Critic 
	- clip 
	- Reference model are needed

## Naive approach
**only using reward**


- Absolute scores mean higher score gets more reward
- Analogy : The teacher grades our exams and gives an â€œabsolute score.â€ I typically score above 80 out of 100, while my brother often gets around 30. We then take these scores directly to our dad to ask for pocket moneyâ€”meaning our â€œrewardâ€ (in RL terms) is simply our raw exam score. Whoever gets a higher score receives more pocket money.
- *problem* : 
	- Unfairness : if my brother improves from 30 to 60 points through a lot of hard work, he still pales in comparison to my usual 80+. He doesnâ€™t get the encouragement he deserves.
	- Instability : Chasing higher scores myself could lead me to extreme study methods (e.g., cramming at all hours, staying up very late). Sometimes I might get 95, other times only 60, so my scoreâ€”and hence the reward signalâ€”fluctuates dramatically.
- **using absolute scores as Reward**Â causes large reward fluctuations, and my brother ends up feeling itâ€™s not worth trying to improve in small increments.


### Mathematical Correspondence

$Jnaiveâ€‹(Î¸)=E(q,o)âˆ¼(data,Ï€Î¸â€‹)â€‹[r(o)],$

- **â€œoptimize only the final reward,â€**
- **high variance and insufficient incentives for partial improvements.**


Actor lacks aÂ **baseline**Â that matches its own current level, and that hinders training efficiency.







## Introducing the Critic: Using a â€œPredicted Score Lineâ€ to Improve Rewards

***itâ€™s not just about the absolute score; itâ€™s about how much youâ€™ve improved relative to your current level.***


- so it is decided to *- Set my â€œpredicted score lineâ€ at 80 points and my brotherâ€™s at 40. If we exceed these lines on an exam, we get more pocket money; if not, we get very little or none.*


- higher increment from their respective selected baselines means higher reward gains and less increment mean less more reward

- encourages each person to improve on their own baseline instead of based on absolute scores

- the baselines are also **readjusted** as the children progress

### Mathematical Correspondence

- this â€œ**score line**â€ is known as theÂ **value function**,Â 
- $$VÏˆâ€‹(s). $$
- It acts as a baseline. Our training objective evolves from â€œjust rewardâ€ to â€œhow much we outperform that baseline,â€
- expressed by the Advantage:


					$Atâ€‹=rtâ€‹âˆ’VÏˆâ€‹(stâ€‹).$

- For a given stateÂ $$st$$â€‹Â and actionÂ $$otâ€‹$$
if the actual **reward exceeds the Criticâ€™s expectation**, it means the action **performed better than predicted**.
 
If itâ€™s **lower**, that action **underperformed**


Formula

$$Jadvâ€‹(Î¸)=E[A(o)],whereÂ A(o)=r(o)âˆ’VÏˆâ€‹(o).$$


By subtracting this â€œscore line,â€ 
- we reduce variance in training,
- giving higher gradient signals to actions that exceed expectations and 
- penalizing those that fall short.



## Adding  Clip & Min: Keeping Learning in Check

> ðŸ§  **Analogy**: A single great test score shouldnâ€™t trigger a complete change in how I study.

- Imagine I score 100 on one exam. My dad doesnâ€™t instantly double my allowance â€” that would encourage erratic behavior.
    
- Instead, he limits how much he rewards me, keeping things **steady and controlled**.
    

### PPO Mechanism: Clipping

- PPO controls policy updates using a **clip function**:
    
$$    min(rtâ€‹(Î¸)Atâ€‹,Â clip(rtâ€‹(Î¸),1âˆ’Ïµ,1+Ïµ)Atâ€‹)$$
- Where 
$$ rtâ€‹(Î¸)=\frac{Ï€Î¸â€‹(otâ€‹âˆ£stâ€‹)}{Ï€Î¸oldâ€‹â€‹(otâ€‹âˆ£stâ€‹)â€‹} $$
- is the ratio of the new and old policy probabilities.
    ``
- If this ratio deviates too much, itâ€™s **clipped**, ensuring stable updates.
    

### Benefit:

Prevents overconfidence or drastic changes from a single good outcome.

---

##  Reference Model: Preventing Cheating

> ðŸ§  **Analogy**: Even if I score high, I can't cheat or manipulate the teacher.

- If I start using unfair tactics just to get good grades, Dad steps in.
    
- He compares my behavior to how I used to study honestly and punishes extreme shifts.
    

### PPO Mechanism: KL Penalty

- Adds a penalty if the current policy strays too far from the **reference policy**:
    
    $$-\beta \, D_{KL}(\pi_\theta \, \| \, \pi_{\text{ref}})$$
- This acts like a **"cheating detector"**, ensuring the model stays close to its original intent.
    

###  Benefit:

Maintains ethical, aligned outputs and prevents policy drift.

---

## GRPO: Removing the Critic via Multiple Simulated Averages
or
**Replacing the Value Function with â€œMultiple Simulated Averages**


> ðŸ§  **Analogy**: Instead of Dad estimating how I should do, I take 5 practice tests and use the **average score** as my benchmark.

- PPO relies on a Critic (Dad) to evaluate each action.
    
- In LLMs, the Critic is expensive and hard to train for delayed rewards (e.g., final answer score).
    

### ðŸ”§ GRPO Mechanism:

- Replace the Critic with **multiple outputs from the old policy**.
    
- Compute the **average reward** from these outputs.
    
- Use this as the baseline:


- According to DeepSeekMathâ€™s technical report, the GRPO objective (omitting some symbols) is:

$$JGRPOâ€‹(Î¸)=E[â€‹i=1âˆ‘Gâ€‹(min(\frac{Ï€Î¸â€‹(oiâ€‹)}{Ï€Î¸oldâ€‹â€‹(oiâ€‹)}â€‹Aiâ€‹ ,Â clip(\frac{Ï€Î¸â€‹(oiâ€‹)}{Ï€Î¸oldâ€‹â€‹(oiâ€‹)}â€‹,1âˆ’Îµ,1+Îµ)Aiâ€‹)âˆ’Â Î²Â DKLâ€‹(Ï€Î¸â€‹Â âˆ¥Â Ï€refâ€‹))],â€‹$$
    where
    $$Ai=\frac{r_i - \text{mean}(r_1, \ldots, r_G)}{\text{std}(r_1, \ldots, r_G)}$$



PPO relies on the Actor + Critic + Clip + KL penalty framework. However, in large language model (LLM) scenarios, the Critic (value function)Â **often needs to be as large as the Actor**Â to accurately evaluate states, which can be costly and sometimes impracticalâ€”especially if you only have a single final reward at the end (like a final answer quality).

Hence,Â **Group Relative Policy Optimization (GRPO)**Â steps in. Its core idea:

- **No separate value network**Â for the Critic,
- Sample multiple outputs from the old policy for the same question or state,
- **Treat the average reward of these outputs as the baseline**,
- Anything above average yields a â€œpositive advantage,â€ anything below yields a â€œnegative advantage.â€

Meanwhile, GRPOÂ **retains**Â PPOâ€™s Clip and KL mechanisms to ensure stable, compliant updates. But drops the need for a separate value function.
    
### Benefit:

Reduces memory and compute cost while preserving stable learning.

---

## Summary: From PPO to GRPO

- **raw absolute scores**Â to PPOâ€™s full mechanism (Critic, Advantage, Clip, Reference Model), and finally toÂ **GRPO**Â (leveraging multiple outputsâ€™ average scores to eliminate the value function).


- **Role of the Critic**: Provides a â€œreasonable expectationâ€ for each state, significantly reducing training variance.
- **Clip & min Mechanism**: Constrains the update magnitude, preventing overreacting to a single â€œbreakthroughâ€ exam.
- **Reference Model**: Discourages â€œcheatingâ€ or extreme deviations, ensuring the policy remains reasonably aligned with its initial state.
- **Advantages of GRPO**: In large language models, it removes the need for a separate value network, reducing memory and compute costs while aligning well with â€œcomparativeâ€ Reward Model designs.

- GRPO avoids maintaining a massive Critic while still offering a relative reward signal. It preserves the stability and compliance features of PPO but streamlines the process.

|Feature|PPO|GRPO|
|---|---|---|
|Critic (Value Function)|Required|Not Required|
|Advantage Estimation|At=râˆ’V(s)A_t = r - V(s)Atâ€‹=râˆ’V(s)|Average over multiple old policy outputs|
|Update Control|Clip + Min|Clip + Min|
|Reference Alignment|KL Penalty|KL Penalty|
|Analogy|Dad tracks study & score lines|Kids simulate exams; Dad uses average score|

---

## ðŸ Final Thought:

GRPO keeps the strengths of PPO (safe updates, ethical alignment) but simplifies training by letting models **self-evaluate using simulated outputs**. Itâ€™s like letting students run mock exams and rewarding them only if they beat their own average â€” no need for a full-time evaluator.













---

[PAPER]()
## PAPER SUMMARY




---

[trl grpo trainer]()

## understanding the functions plus maths

excercise :


here's the exercise - 
i hope we're all acquainted with the GRPO loss. 
this is the `grpo_trainer` from HF, the code containing the GRPOTrainer class - https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py

2 exercises from  the code - 
1> what is the shape of `prompt_ids` from the code.
2> based on what's present in the code, how would you implement the DAPO loss - https://arxiv.org/pdf/2503.14476


